{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alison's approach to regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRoss-Industry Standard Process for Data Mining (CRISP-DM)\n",
    "\n",
    "Before we dig into the problem, lets refresh our memories on the steps in the CRISP-DM model.\n",
    "\n",
    "<img src=\"img/new_crisp-dm.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "<img src=\"img/grocery-cart.jpg\" width=\"500\">\n",
    "\n",
    "The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities.\n",
    "\n",
    "The data is located in the csv called `big_mart.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously explored the features of the big_mart dataset but now BigMart wants us to answer the following question:\n",
    "\n",
    "**The sales team at BigMart HQ wants to know how to improve sales at their stores. They have sales data from different BigMart stores that records both item and store information. They have asked you to build a model to help predict which products at which stores sell well using the BigMart dataset.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data Understanding\n",
    "\n",
    "We have already done a great deal of exploratory data analysis of this dataset.  Let's refresh out memory on what the original data contained. \n",
    "\n",
    "<img src=\"img/big_mart_data_variables.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Data Preparation\n",
    "\n",
    "This step has already been done for you.  The following steps were taken and are reflected in the `big_mart_clean.csv` file.\n",
    "\n",
    "- Imputed missing values for `Outlet_Size` (replaced missing with the mode)\n",
    "- Imputed missing values for `Item_Weight` (replaced missing with the average)\n",
    "- Cleaned typos in `Item_Fat_Content`\n",
    "- Created new variable `Item_Type_Combined` (labels items as food, non-consumable, or drinks)\n",
    "- Created new variable `Outlet_Years` (the years of operation of a store)\n",
    "\n",
    "\n",
    "#### Before we begin modeling let's do a quick exploration of out new dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels\n",
    "import scipy\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "big_mart = pd.read_csv('big_mart_clean.csv')\n",
    "big_mart_orig = pd.read_csv('big_mart.csv')\n",
    "big_mart.drop(columns=['Item_Sales_Cat'])\n",
    "big_mart_reg = pd.concat([big_mart.drop(columns=['Item_Sales_Cat']), big_mart_orig['Item_Outlet_Sales']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set random state for our notebook\n",
    "import numpy as np\n",
    "np.random.seed(217)\n",
    "\n",
    "y = big_mart_reg['Item_Outlet_Sales']\n",
    "X = big_mart_reg.drop(columns=['Item_Outlet_Sales', 'Item_Identifier'])\n",
    "\n",
    "#split data into test and train sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "numeric_features = ['Item_Weight', 'Item_Visibility',\n",
    "                     'Item_MRP', 'Outlet_Years']\n",
    "ss=StandardScaler()\n",
    "\n",
    "categorical_features = [ 'Item_Fat_Content',\n",
    "       'Item_Type', 'Outlet_Identifier',\n",
    "       'Outlet_Size', 'Outlet_Location_Type',\n",
    "       'Outlet_Type', 'Item_Type_Combined' ]\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"ordinal\", OrdinalEncoder()),\n",
    "    ('onehot', OneHotEncoder(categories='auto',drop='first'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', ss, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)], remainder='passthrough')\n",
    "\n",
    "#let's use it now!\n",
    "\n",
    "X_train_tran = preprocessor.fit_transform(X_train)\n",
    "X_train_tran\n",
    "\n",
    "#and we can transform X_test too\n",
    "\n",
    "X_test_tran = preprocessor.transform(X_test)\n",
    "X_test_tran\n",
    "\n",
    "#get column names for categories\n",
    "cat_names = preprocessor.named_transformers_['cat'].named_steps['ordinal'].categories_\n",
    "\n",
    "cat_names = [val for sublist in cat_names for val in sublist[1:]]\n",
    "cat_names\n",
    "\n",
    "#full list of column names\n",
    "column_names = numeric_features + cat_names\n",
    "\n",
    "#apply column names to dataframe\n",
    "X_train_trans = pd.DataFrame(X_train_tran.toarray(), columns=column_names)\n",
    "X_train_trans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Modeling\n",
    "\n",
    "Once we have clean data, we can begin modeling! Remember, modeling, as with any of these other steps, is an iterative process. During this stage, we'll try to build and tune models to get the highest performance possible on our task.\n",
    "\n",
    "Consider the following questions during the modeling step:\n",
    "\n",
    "- Is this a classification task? A regression task? Something else?\n",
    "- What models will we try?\n",
    "- How do we deal with overfitting?\n",
    "- Do we need to use regularization or not?\n",
    "- What sort of validation strategy will we be using to check that our model works well on unseen data?\n",
    "- What loss functions will we use?\n",
    "- What threshold of performance do we consider as successful?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy Model\n",
    "\n",
    "First we are going to start with a dummy model to predict if the product has high or low sales. In our dummy model we classify everything as the majority class.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DummyClassifier to predict only target 0\n",
    "dummy = DummyRegressor(strategy='mean')\n",
    "dummy.fit(X_train_tran, y_train)\n",
    "y_hat_train = dummy.predict(X_train_tran)\n",
    "r2_score(y_hat_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two approaches\n",
    "##### Sklearn vs statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "lin_mod1 = LinearRegression()\n",
    "\n",
    "lin_mod1.fit(X_train_trans,y_train)\n",
    "lin_mod1.score(X_train_trans,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trans.Outlet_Years.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=zip(X_train_trans.columns,lin_mod1.coef_), columns = ['variables', 'coefficients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.Outlet_Type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_terms=PolynomialFeatures(interaction_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_terms.get_feature_names(X_train_trans.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_vars = more_terms.fit_transform(X_train_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_vars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_mod2 = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_mod2.fit(more_vars,y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_mod2.score(more_vars,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Evalution\n",
    "\n",
    "During the evaluation step we want to evaluate the results of our models, and decide the next steps in selecting the \"best\" model.  During this step we should consider the following:\n",
    "\n",
    "- Does our model solve the business problem?\n",
    "- What metrics should we be using to evaluate the \"success\" of our model?\n",
    "- Can we further improve our models?\n",
    "- Do we need more data?  Or different data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helpful code for me later on\n",
    "\n",
    "sns.pairplot(df)b\n",
    "plt.show()\n",
    "fig = sm.qqplot(normal_rv, line = 'r')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
